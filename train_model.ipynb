{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Install and set up required libraries"
      ],
      "metadata": {
        "id": "_RfMVLamOjEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install --upgrade jax jaxlib flax optax pandas scikit-learn Pillow\n",
        "\n",
        "# Check GPU availability\n",
        "import jax\n",
        "print(\"Available devices:\", jax.devices())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPz7dP4dOqPJ",
        "outputId": "4d7eee1e-2ddf-4cd7-ca13-ecad1a02d084"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (0.4.38)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (0.4.38)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (0.10.2)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (0.2.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (11.1.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from jax) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from jax) (1.26.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.10/dist-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax) (1.13.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax) (1.1.0)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax) (0.6.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.71)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax) (13.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax) (6.0.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.10/dist-packages (from optax) (0.1.88)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.10/dist-packages (from optax) (1.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.87->optax) (0.12.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (2.18.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (4.25.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (4.11.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epy]->optax) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epy]->optax) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epy]->optax) (3.21.0)\n",
            "Available devices: [CudaDevice(id=0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Preparing the dataset"
      ],
      "metadata": {
        "id": "UuHdWh67PSfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_file = \"skeletonized_image.zip\"\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")\n",
        "\n",
        "csv_file = \"/content/skeletonized_labels.csv\"\n",
        "base_dir = \"/content/skeletonized_image\""
      ],
      "metadata": {
        "id": "b7ZetDtjPDet"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Data loading and preprocessing"
      ],
      "metadata": {
        "id": "3IaDwFWUOuFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_data_from_csv(csv_file, img_size=(28, 28)):\n",
        "    \"\"\"\n",
        "    Load and preprocess skeletonized images from a CSV file.\n",
        "\n",
        "    Args:\n",
        "        csv_file (str): Path to the CSV file containing image paths and labels.\n",
        "        img_size (tuple): Target size for resizing the images.\n",
        "\n",
        "    Returns:\n",
        "        images (np.ndarray): Array of preprocessed images.\n",
        "        labels (np.ndarray): Array of corresponding labels.\n",
        "    \"\"\"\n",
        "    # Read the CSV file\n",
        "    data = pd.read_csv(csv_file)\n",
        "\n",
        "    images, labels = [], []\n",
        "    for _, row in data.iterrows():\n",
        "        img_path = row['file_path']  # Image path from CSV\n",
        "        label = row['label']        # Label from CSV\n",
        "\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
        "            img = img.resize(img_size)  # Resize to target size\n",
        "            images.append(np.array(img) / 255.0)  # Normalize pixel values\n",
        "            labels.append(label)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "\n",
        "    return np.array(images, dtype=np.float32), np.array(labels, dtype=np.int32)\n",
        "\n",
        "# Set paths\n",
        "csv_file = \"/content/skeletonized_labels.csv\"  # Path to the CSV file\n",
        "\n",
        "# Load data\n",
        "images, labels = load_data_from_csv(csv_file)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "    images, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Add batch dimension (JAX expects [N, H, W, C] format)\n",
        "train_images = train_images[..., np.newaxis]\n",
        "test_images = test_images[..., np.newaxis]\n",
        "\n",
        "print(f\"Training images: {train_images.shape}, Training labels: {train_labels.shape}\")\n",
        "print(f\"Testing images: {test_images.shape}, Testing labels: {test_labels.shape}\")\n",
        "\n",
        "print(\"Unique labels in dataset:\", np.unique(labels))\n",
        "print(\"Number of unique labels:\", len(np.unique(labels)))\n",
        "\n",
        "num_classes = len(np.unique(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcPF_qHwPIJs",
        "outputId": "54cefb64-a717-4a1b-ef4d-b182e34200a9"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images: (498, 28, 28, 1), Training labels: (498,)\n",
            "Testing images: (125, 28, 28, 1), Testing labels: (125,)\n",
            "Unique labels in dataset: [0 1 2 3 4 5 6 7 8 9]\n",
            "Number of unique labels: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. CNN model definition"
      ],
      "metadata": {
        "id": "a4MQi3EFPr9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "\n",
        "# Define the CNN model\n",
        "class SkeletonCNN(nn.Module):\n",
        "    num_classes: int = len(np.unique(labels))\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        # 기존 첫 번째 Conv: 채널 32 -> 64로 늘림\n",
        "        x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "\n",
        "        # 기존 두 번째 Conv: 채널 64 -> 128로 늘림\n",
        "        x = nn.Conv(features=128, kernel_size=(3, 3))(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "\n",
        "        # **새롭게 추가한** 세 번째 Conv\n",
        "        x = nn.Conv(features=128, kernel_size=(3, 3))(x)\n",
        "        x = nn.relu(x)\n",
        "        # 필요하면 pooling을 한 번 더 넣을 수도 있음\n",
        "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "\n",
        "        # Flatten\n",
        "        x = x.reshape((x.shape[0], -1))\n",
        "\n",
        "        # Dense layer\n",
        "        x = nn.Dense(features=256)(x)  # 기존 128 -> 256로 증가\n",
        "        x = nn.relu(x)\n",
        "\n",
        "        # Output layer\n",
        "        x = nn.Dense(features=self.num_classes)(x)\n",
        "        return x\n",
        "\n",
        "model = SkeletonCNN(num_classes=10)\n",
        "rng = jax.random.PRNGKey(0)\n",
        "rng, init_rng = jax.random.split(rng)\n",
        "\n",
        "params = model.init(init_rng, jnp.ones([1, 28, 28, 1]))['params']\n"
      ],
      "metadata": {
        "id": "c-X7D39VPvNA"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Training and Evaluation Functions\n"
      ],
      "metadata": {
        "id": "cfMcZDRCPxkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Training Step"
      ],
      "metadata": {
        "id": "-8LofWrwP0Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def train_step(state, batch):\n",
        "    def loss_fn(params):\n",
        "        logits = SkeletonCNN().apply({'params': params}, batch['image'])\n",
        "        loss = jnp.mean(optax.softmax_cross_entropy(\n",
        "            logits=logits,\n",
        "            labels=jax.nn.one_hot(batch['label'], num_classes=num_classes)))\n",
        "        return loss, logits\n",
        "\n",
        "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    (_, logits), grads = grad_fn(state.params)\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    metrics = compute_metrics(logits, batch['label'])\n",
        "    return state, metrics\n",
        "\n",
        "@jax.jit\n",
        "def eval_step(params, batch):\n",
        "    logits = SkeletonCNN().apply({'params': params}, batch['image'])\n",
        "    return compute_metrics(logits, batch['label'])\n",
        "\n",
        "def compute_metrics(logits, labels):\n",
        "    loss = jnp.mean(optax.softmax_cross_entropy(\n",
        "        logits, jax.nn.one_hot(labels, num_classes=num_classes)\n",
        "    ))\n",
        "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
        "    return {'loss': loss, 'accuracy': accuracy}"
      ],
      "metadata": {
        "id": "sFQY7hW1P12B"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Training Loop"
      ],
      "metadata": {
        "id": "IktZ06SJP4uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(state, images, labels, batch_size, rng):\n",
        "    num_samples = images.shape[0]\n",
        "    perms = jax.random.permutation(rng, num_samples)\n",
        "    steps_per_epoch = num_samples // batch_size  # remainder는 버린다. (원한다면 유지해도 됨)\n",
        "\n",
        "    batch_metrics = []\n",
        "    for i in range(steps_per_epoch):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = start_idx + batch_size\n",
        "\n",
        "        batch = {\n",
        "            'image': images[perms[start_idx:end_idx], ...],\n",
        "            'label': labels[perms[start_idx:end_idx], ...]\n",
        "        }\n",
        "        state, metrics = train_step(state, batch)\n",
        "        batch_metrics.append(metrics)\n",
        "\n",
        "    batch_metrics = jax.device_get(batch_metrics)\n",
        "    epoch_metrics = {\n",
        "        k: np.mean([m[k] for m in batch_metrics])\n",
        "        for k in batch_metrics[0]\n",
        "    }\n",
        "    return state, epoch_metrics"
      ],
      "metadata": {
        "id": "xzzqf-bbP81p"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Model Initialization and Training\n"
      ],
      "metadata": {
        "id": "q3ppu-X0QBl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rng = jax.random.PRNGKey(0)\n",
        "rng, init_rng = jax.random.split(rng)\n",
        "\n",
        "model = SkeletonCNN()\n",
        "params = model.init(init_rng, jnp.ones([1, 28, 28, 1]))['params']\n",
        "\n",
        "# Adam\n",
        "tx = optax.adam(learning_rate=1e-3)\n",
        "\n",
        "state = train_state.TrainState.create(\n",
        "    apply_fn=model.apply,\n",
        "    params=params,\n",
        "    tx=tx\n",
        ")\n",
        "\n",
        "num_epochs = 20\n",
        "batch_size = 10\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    rng, input_rng = jax.random.split(rng)\n",
        "    state, train_metrics = train_epoch(\n",
        "        state, train_images, train_labels,\n",
        "        batch_size, input_rng\n",
        "    )\n",
        "    test_metrics = eval_step(\n",
        "        state.params, {'image': test_images, 'label': test_labels}\n",
        "    )\n",
        "    print(f\"[Epoch {epoch:2d}] \"\n",
        "          f\"Train Loss={train_metrics['loss']:.4f}, Train Acc={train_metrics['accuracy']:.2%}, \"\n",
        "          f\"Test Loss={test_metrics['loss']:.4f}, Test Acc={test_metrics['accuracy']:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRizw9epQEDG",
        "outputId": "c791aa00-fab0-4b15-98c6-5093f9496afd"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch  1] Train Loss=2.3187, Train Acc=7.96%, Test Loss=2.2999, Test Acc=11.20%\n",
            "[Epoch  2] Train Loss=2.3005, Train Acc=11.84%, Test Loss=2.3425, Test Acc=11.20%\n",
            "[Epoch  3] Train Loss=2.2237, Train Acc=15.10%, Test Loss=2.1139, Test Acc=19.20%\n",
            "[Epoch  4] Train Loss=2.0119, Train Acc=18.37%, Test Loss=1.8548, Test Acc=24.80%\n",
            "[Epoch  5] Train Loss=1.7518, Train Acc=30.20%, Test Loss=1.6721, Test Acc=35.20%\n",
            "[Epoch  6] Train Loss=1.5792, Train Acc=38.78%, Test Loss=1.4736, Test Acc=36.80%\n",
            "[Epoch  7] Train Loss=1.4243, Train Acc=44.29%, Test Loss=1.3341, Test Acc=49.60%\n",
            "[Epoch  8] Train Loss=1.2428, Train Acc=52.04%, Test Loss=1.2500, Test Acc=50.40%\n",
            "[Epoch  9] Train Loss=1.1186, Train Acc=57.55%, Test Loss=1.0429, Test Acc=62.40%\n",
            "[Epoch 10] Train Loss=0.9751, Train Acc=63.06%, Test Loss=0.9429, Test Acc=67.20%\n",
            "[Epoch 11] Train Loss=0.7998, Train Acc=67.14%, Test Loss=0.7763, Test Acc=79.20%\n",
            "[Epoch 12] Train Loss=0.6548, Train Acc=75.71%, Test Loss=0.7095, Test Acc=75.20%\n",
            "[Epoch 13] Train Loss=0.5358, Train Acc=82.04%, Test Loss=0.7979, Test Acc=67.20%\n",
            "[Epoch 14] Train Loss=0.5640, Train Acc=79.80%, Test Loss=0.7447, Test Acc=78.40%\n",
            "[Epoch 15] Train Loss=0.4503, Train Acc=84.69%, Test Loss=0.6279, Test Acc=85.60%\n",
            "[Epoch 16] Train Loss=0.3232, Train Acc=88.98%, Test Loss=0.4972, Test Acc=88.80%\n",
            "[Epoch 17] Train Loss=0.3068, Train Acc=89.80%, Test Loss=0.7051, Test Acc=83.20%\n",
            "[Epoch 18] Train Loss=0.2771, Train Acc=90.20%, Test Loss=0.5190, Test Acc=88.00%\n",
            "[Epoch 19] Train Loss=0.2454, Train Acc=91.02%, Test Loss=0.5016, Test Acc=86.40%\n",
            "[Epoch 20] Train Loss=0.2558, Train Acc=92.04%, Test Loss=0.5010, Test Acc=90.40%\n"
          ]
        }
      ]
    }
  ]
}